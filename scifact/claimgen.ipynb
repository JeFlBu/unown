{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Smr0ok1lbXQJ"
   },
   "source": [
    "# Claim Generation, SciFact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import transformers\n",
    "from datasets import load_dataset, load_metric, DatasetDict, load_from_disk, Dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import hashlib\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get random evidence (instead of gold one), set to True\n",
    "randomize_from_fileseed=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lxA4ie8ZXTUu"
   },
   "outputs": [],
   "source": [
    "#Path for multivers clone\n",
    "base_path_multivers='/homes//feverous_work/multivers/'\n",
    "\n",
    "#Path for output of experiments\n",
    "base_path='/homes//feverous_work/feverousdata/'\n",
    "\n",
    "corpus_path='data_train/target/scifact_10/corpus.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scifact data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=dict()\n",
    "corpusjsonf=open(base_path_multivers+corpus_path)\n",
    "\n",
    "#Converting the corpus to a dictionary\n",
    "for line  in corpusjsonf:\n",
    "    elt=json.loads(line)\n",
    "    corpus[elt['doc_id']]={'title':elt['title'],'abstract':elt['abstract']}\n",
    "corpusjsonf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the seed claims to be used for training\n",
    "train_corp_sci_path='data_train/target/scifact_10_original/claims_train.jsonl'\n",
    "train_corp_sci_f=open(base_path_multivers+train_corp_sci_path)\n",
    "train_corp_sci=[]\n",
    "for line in train_corp_sci_f:\n",
    "    train_corp_sci+=[json.loads(line)]\n",
    "    \n",
    "train_corp_sci_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove claims with no evidence\n",
    "train_corp_sci_filtered=[x for x in train_corp_sci if len(list(x['evidence']))>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corp_sci_filtered_pos=[]\n",
    "train_corp_sci_filtered_neg=[]\n",
    "nbevidencestats=dict()\n",
    "#we split train between supports and refutes, and we get some statistics\n",
    "for elt in train_corp_sci_filtered:\n",
    "    labelforelt=''\n",
    "    nbevhere=0\n",
    "    for ev in elt['evidence'].keys():\n",
    "        for evinp in elt['evidence'][ev]:\n",
    "            nbevhere+=1\n",
    "            if labelforelt=='error':\n",
    "                continue\n",
    "            elif labelforelt=='':\n",
    "                labelforelt=evinp['label']\n",
    "            elif not labelforelt==evinp['label']:\n",
    "                print('problem')\n",
    "                labelforelt='error'\n",
    "    if nbevhere in nbevidencestats.keys():\n",
    "        nbevidencestats[nbevhere]+=1\n",
    "    else:\n",
    "        nbevidencestats[nbevhere]=1\n",
    "    elt['nbevhere']=nbevhere\n",
    "    if labelforelt=='CONTRADICT':\n",
    "        train_corp_sci_filtered_neg+=[elt]\n",
    "    elif labelforelt=='SUPPORT':\n",
    "        train_corp_sci_filtered_pos+=[elt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 355, 2: 200, 3: 79, 4: 35, 5: 8, 9: 2, 7: 3, 11: 2, 6: 5, 8: 4}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbevidencestats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "693"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corp_sci_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corp_sci_filtered_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corp_sci_filtered_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_sci(doc_id,sentence_nb):\n",
    "    #Get all the sentences in one document\n",
    "    return corpus[doc_id]['abstract'][sentence_nb]\n",
    "\n",
    "def get_nb_sentences_sci(doc_id):\n",
    "    #Get the number of sentences in one document\n",
    "    return len(corpus[doc_id]['abstract'])\n",
    "\n",
    "def get_title_sci(doc_id):\n",
    "    #Get title of a document\n",
    "    return corpus[doc_id]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "respos=[]\n",
    "resneg=[]\n",
    "allevspos=[]\n",
    "allevsneg=[]\n",
    "res_for_modelpos=[]\n",
    "res_for_modelneg=[]\n",
    "#We build our evidence set for claim generation\n",
    "for data,res,allevs,res_for_model in [[train_corp_sci_filtered_pos,respos,allevspos,res_for_modelpos],[train_corp_sci_filtered_neg,resneg,allevsneg,res_for_modelneg]]:\n",
    "    for elttmp in data:\n",
    "        \n",
    "        elt=copy.deepcopy(elttmp)\n",
    "        evfortxtgen=[]\n",
    "        title=[]\n",
    "        for page in elt['evidence'].keys():\n",
    "            title+=[get_title_sci(int(page))]\n",
    "            for ev in range(len(elt['evidence'][page])):\n",
    "                lenevtoadd=len(elt['evidence'][page][ev]['sentences'])\n",
    "                nbsentencestot=get_nb_sentences_sci(int(page))\n",
    "                #If we have to randomize, we select random evidence from document with the same number as original\n",
    "                if randomize_from_fileseed:\n",
    "                    elt['evidence'][page][ev]['sentences']=random.sample(list(range(nbsentencestot)),lenevtoadd)\n",
    "                for evtoadd in elt['evidence'][page][ev]['sentences']:\n",
    "                    #from id to text\n",
    "                    evfortxtgen+=[get_sentence_sci(int(page),evtoadd)]\n",
    "                \n",
    "        title=' | '.join(title)\n",
    "        elt['claim']='TODO'\n",
    "        elt['title']=title\n",
    "        elt['evfortxtgen']=evfortxtgen\n",
    "        res_for_model+=[{\"evidences_txt\":evfortxtgen, \"claim\":'', \"title\":elt['title']}]\n",
    "        res+=[elt]\n",
    "        allevs+=[evfortxtgen]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_for_modelpos=res_for_modelpos[:237]\n",
    "res_for_modelneg=res_for_modelneg[:237]\n",
    "respos=respos[:237]\n",
    "resneg=resneg[:237]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspos=Dataset.from_pandas(pd.DataFrame(data=res_for_modelpos))\n",
    "dsneg=Dataset.from_pandas(pd.DataFrame(data=res_for_modelneg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyds=Dataset.from_pandas(pd.DataFrame(data=[{\"evidences_txt\":'', \"claim\":'', \"title\":''}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasetpos = DatasetDict({\n",
    "    \"train\": dspos,\n",
    "     \"validation\": emptyds,\n",
    "     \"test\": emptyds,\n",
    "})\n",
    "datasetneg = DatasetDict({\n",
    "    \"train\": dsneg,\n",
    "     \"validation\": emptyds,\n",
    "     \"test\": emptyds,\n",
    "})\n",
    "datasetpos.save_to_disk(base_path+'dstopredictfromrandomnewpos')\n",
    "datasetneg.save_to_disk(base_path+'dstopredictfromrandomnewneg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples_train=237 #The number of sanples we want to use for training in make_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_name_from_params(params):\n",
    "    #When we finetune our models or run inference on them,we give them a unique name based on parameters\n",
    "    return 'ds_'+'_'.join([x+'_'+params[x].replace('/','Z').replace('_','T') for x in params.keys() if not(params[x]=='') and not(x in ['output','localmodel','outputdir'])])\n",
    "\n",
    "\n",
    "def generate_hash_file_name_from_params(params):\n",
    "    #As this name can be very long we hash it\n",
    "    hash_object = hashlib.sha1(generate_file_name_from_params(params).encode('utf-8'))\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    return str(hex_dig)\n",
    "\n",
    "def make_sh(params):\n",
    "    #We create a script for running train/inference on our target model\n",
    "    base_str=[\\\n",
    "        '#!/bin/bash', \\\n",
    "        '/home//.conda/envs/feverous2/bin/python main.py \\\\' , \\\n",
    "        '--model_name_or_path '+ ('/data/' if params['localmodel'] else '')+ params['modeltouse'] +' \\\\' , \\\n",
    "        '--dataset_name_local '+ params['dsnamelocal'] +' \\\\' , \\\n",
    "        '--log_level error \\\\' , \\\n",
    "        '--only_train \\\\' , \\\n",
    "        '--output_dir \\'/data/'+ params['outputdir'] +'\\' \\\\' , \\\n",
    "        '--per_device_train_batch_size 16 \\\\' , \\\n",
    "        '--per_device_eval_batch_size 8 \\\\' , \\\n",
    "        '--gradient_accumulation_steps 1 \\\\' , \\\n",
    "        '--max_source_length 512 \\\\' , \\\n",
    "        '--min_target_length 30 \\\\' , \\\n",
    "        '--max_target_length 64 \\\\' , \\\n",
    "        '--generation_max_length 64 \\\\' , \\\n",
    "        '--num_train_epochs 20 \\\\' , \\\n",
    "        '--learning_rate 1e-4 \\\\' , \\\n",
    "        '--save_strategy epoch \\\\' , \\\n",
    "        '--evaluation_strategy epoch \\\\' , \\\n",
    "        '--fp16 \\\\' , \\\n",
    "        '--load_best_model_at_end \\\\' , \\\n",
    "        '--predict_with_generate \\\\' , \\\n",
    "        '--overwrite_output_dir \\\\' , \\\n",
    "        '--metric_for_best_model eval_rouge1 \\\\' , \\\n",
    "        '--save_total_limit 1 \\\\' , \\\n",
    "        '--num_beams 5 \\\\' , \\\n",
    "        '--generation_num_beams 2 \\\\' , \\\n",
    "        '--group_by_length \\\\' , \\\n",
    "        '--sortish_sampler \\\\' , \\\n",
    "        '--weight_decay 0.01 \\\\' , \\\n",
    "        '--label_smoothing_factor 0.1 \\\\' , \\\n",
    "        '--include_inputs_for_metrics \\\\' , \\\n",
    "        '--overwrite_cache \\\\' , \\\n",
    "        '--gradient_checkpointing \\\\' , \\\n",
    "        '--remove_unused_columns \\\\' , \\\n",
    "        '--max_eval_samples 200 \\\\' , \\\n",
    "        '--predict_samples_from_train '+str(nb_samples_train)+' \\\\'] \n",
    "    if 'pathtxt' in params.keys() and not params['pathtxt']=='':\n",
    "        base_str+=['--from_path /data/'+params['pathtxt']+' \\\\']\n",
    "    if 'maxnb' in params.keys() and not params['maxnb']=='':\n",
    "        base_str+=['--max_train_samples '+params['maxnb']+' \\\\']\n",
    "        \n",
    "    if 'change_subset' in params.keys() and not params['change_subset']=='':\n",
    "        base_str+=['--change_subset \\\\']\n",
    "    \n",
    "    if params['step']=='train':\n",
    "        base_str+=['--do_train']\n",
    "    elif params['step']=='test':\n",
    "        base_str+=['--do_predict']\n",
    "    else:\n",
    "        print('error 0')\n",
    "    \n",
    "    return base_str\n",
    "    \n",
    "    \n",
    "def prepare_run_sh(sh_txt,folder_used):\n",
    "    #From make_sh, we need to make the script runnable and run it\n",
    "    f=open(base_path+'run_auto.sh','w')\n",
    "    for elt in sh_txt:\n",
    "        f.write(elt+'\\n')\n",
    "    f.close()\n",
    "    outputs=[]\n",
    "    errors=[]\n",
    "    list_commands=[\n",
    "        [\"chmod a+wrx run_auto.sh\",base_path],\n",
    "        [\"./run_auto.sh\",base_path],\n",
    "\n",
    "    ]\n",
    "    for elt in list_commands:\n",
    "        print('##########')\n",
    "        print(elt)\n",
    "        bashCommand = elt[0]\n",
    "        cwd = elt[1]\n",
    "        process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE,stderr=subprocess.STDOUT, cwd=cwd)\n",
    "        output, error = process.communicate()\n",
    "        outputs+=[output]\n",
    "        errors+=[error]\n",
    "        \n",
    "        print(output)\n",
    "        print(error)\n",
    "    return outputs,error\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_training_claimgenerator(list_execs):\n",
    "    #Function that run all training/inference for a list_execs object containing details about each train/inference\n",
    "    outputs,errors=[],[]\n",
    "    outposname,outnegname='',''\n",
    "    total_time=0\n",
    "    for nb_exec in range(len(list_execs)):\n",
    "        if type(list_execs[nb_exec]['modeltouse'])==int:\n",
    "            if list_execs[nb_exec]['modeltouse']<0:\n",
    "                list_execs[nb_exec]['modeltouse']=nb_exec+list_execs[nb_exec]['modeltouse']\n",
    "            list_execs[nb_exec]['modeltouse']=list_execs[list_execs[nb_exec]['modeltouse']]['outputdir']\n",
    "            \n",
    "            \n",
    "        if type(list_execs[nb_exec]['pathtxt'])==int:\n",
    "            if list_execs[nb_exec]['pathtxt']<0:\n",
    "                list_execs[nb_exec]['pathtxt']=nb_exec+list_execs[nb_exec]['pathtxt']\n",
    "            list_execs[nb_exec]['pathtxt']=list_execs[list_execs[nb_exec]['pathtxt']]['outputdir']+'/predictions.txt'\n",
    "            \n",
    "            \n",
    "        \n",
    "        list_execs[nb_exec]['outputdir']=generate_hash_file_name_from_params(list_execs[nb_exec])\n",
    "        finalname=list_execs[nb_exec]['outputdir']\n",
    "        oldfinalname=generate_file_name_from_params(list_execs[nb_exec])\n",
    "        print(oldfinalname + ' hash : '+str(finalname))\n",
    "        if 'output' in  list_execs[nb_exec].keys():\n",
    "            print('èèèèèè output type:'+list_execs[nb_exec]['output'])\n",
    "        if list_execs[nb_exec]['step']=='test' :\n",
    "            if os.path.exists('/data/'+finalname+'/predictions.txt'):\n",
    "                ####This model is already trained, ok\n",
    "                print(finalname+' already done ')\n",
    "            elif os.path.exists('/data/'+oldfinalname+'/predictions.txt'):\n",
    "                ####This model is already trained, ok\n",
    "                print(finalname+' already done, old input format '+oldfinalname)\n",
    "                list_execs[nb_exec]['outputdir']=oldfinalname\n",
    "                finalname=oldfinalname\n",
    "            else :\n",
    "                sh_txt=make_sh(list_execs[nb_exec])\n",
    "                output,error=prepare_run_sh(sh_txt, finalname)\n",
    "                outputs+=[list_execs[nb_exec],output]\n",
    "                errors+=[list_execs[nb_exec],errors]\n",
    "                if not os.path.exists('/data/'+finalname+'/predictions.txt'):\n",
    "                    ####problem\n",
    "                    print('error 3')\n",
    "                    return -1\n",
    "            f=open('/data/'+finalname+'/predict_results.json')\n",
    "            obj_tmp=json.load(f)\n",
    "            f.close()\n",
    "            total_time+=obj_tmp['predict_runtime']\n",
    "            if 'output' in  list_execs[nb_exec].keys() and list_execs[nb_exec]['output']=='pos':\n",
    "                outposname='/data/'+finalname+'/predictions.txt'\n",
    "            if 'output' in  list_execs[nb_exec].keys() and list_execs[nb_exec]['output']=='neg':\n",
    "                outnegname='/data/'+finalname+'/predictions.txt'\n",
    "            \n",
    "        elif list_execs[nb_exec]['step']=='train':\n",
    "            if os.path.exists('/data/'+finalname+'/pytorch_model.bin'):\n",
    "                ####This model is already trained, ok\n",
    "                print(finalname+' already done ')\n",
    "            elif os.path.exists('/data/'+oldfinalname+'/pytorch_model.bin'):\n",
    "                ####This model is already trained, ok\n",
    "                print(finalname+' already done, old input format '+oldfinalname)\n",
    "                list_execs[nb_exec]['outputdir']=oldfinalname\n",
    "                finalname=oldfinalname\n",
    "            else:\n",
    "                \n",
    "                sh_txt=make_sh(list_execs[nb_exec])\n",
    "                output,error=prepare_run_sh(sh_txt, finalname )\n",
    "                outputs+=[list_execs[nb_exec],output]\n",
    "                errors+=[list_execs[nb_exec],errors]\n",
    "                if not os.path.exists('/data/'+finalname+'/pytorch_model.bin'):\n",
    "                    ####problem\n",
    "                    print('error 3')\n",
    "                    return -1\n",
    "            f=open('/data/'+finalname+'/all_results.json')\n",
    "            obj_tmp=json.load(f)\n",
    "            f.close()\n",
    "            total_time+=obj_tmp['train_runtime']\n",
    "            \n",
    "        else:\n",
    "            print('error 1')\n",
    "            return -1\n",
    "    print('list_execs')\n",
    "    print(list_execs)\n",
    "    print('outputs')\n",
    "    print(outputs)\n",
    "    print('errors')\n",
    "    print(errors)\n",
    "    return outposname,outnegname, total_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniaue subsetname for our experiments, used to retrain again on model on a different dataset with identical name\n",
    "subsetname='sci_claims_randomevbartneg_cold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_possibilities=[]\n",
    "\n",
    "\n",
    "#Warm fever_ft_all True, nb_to_train_on_feverous_all 100\n",
    "#Cold fever_ft_all False, nb_to_train_on_feverous_all 0\n",
    "\n",
    "for fever_ft_all in [True]: #Cold set thisto False, Warm set to True\n",
    "    #True,False\n",
    "    #Run a First training on  Fever\n",
    "    for nb_to_train_on_feverous_all in['100']: #0 =Cold, Warm set to 100\n",
    "        # ['0','10','100','']:#''==all\n",
    "        for positive_model in ['bartlarge']:\n",
    "            #bartlarge or 'bartlarge-xsum'\n",
    "            positive_model_real={'bartlarge':'facebook/bart-large','bartlarge-xsum':'facebook/bart-large-xsum'}[positive_model]\n",
    "            for negative_model in ['bartbase-WikiFactEnglish']:\n",
    "                #'bartbase-WikiFactEnglish','bartlarge+bartneg','bartlarge'\n",
    "                negative_model_real={'bartlarge':'facebook/bart-large','bartbase-WikiFactEnglish':'minwhoo/bart-base-negative-claim-generation','bartlarge+bartneg':'minwhoo/bart-base-negative-claim-generation'}[negative_model]\n",
    "                if negative_model in ['bartlarge','bartlarge+bartneg'] and not positive_model=='bartlarge':\n",
    "                    continue\n",
    "                if fever_ft_all==True and positive_model=='bartlarge-xsum':\n",
    "                    continue\n",
    "                if negative_model=='bartbase-WikiFactEnglish' and not positive_model=='bartlarge':\n",
    "                    continue\n",
    "                tmp=[]\n",
    "                if fever_ft_all:\n",
    "                    tmp+=[\n",
    "                        {'modeltouse':positive_model_real,'dsnamelocal':'fever_traindev_10k_pos','pathtxt':'','maxnb':'','step':'train', 'localmodel':False},\n",
    "                        {'modeltouse':negative_model_real,'dsnamelocal':'fever_traindev_10k_neg','pathtxt':'','maxnb':'','step':'train', 'localmodel':False},\n",
    "                    ]\n",
    "                if not nb_to_train_on_feverous_all=='0':\n",
    "                    tmp+=[\n",
    "                        {'modeltouse':0 if fever_ft_all else positive_model_real,'dsnamelocal':'feverous_pos','pathtxt':'','maxnb':nb_to_train_on_feverous_all,'step':'train', 'localmodel': fever_ft_all},\n",
    "                        {'modeltouse':-1,'dsnamelocal':'dstopredictfromrandomnewpos','pathtxt':'','maxnb':'','step':'test','output':'pos','localmodel':True, 'change_subset':subsetname}]\n",
    "                else:\n",
    "                    tmp+=[\n",
    "                        {'modeltouse':0 if fever_ft_all else positive_model_real,'dsnamelocal':'dstopredictfromrandomnewpos','pathtxt':'','maxnb':'','step':'test','output':'pos','localmodel':fever_ft_all, 'change_subset':subsetname}]\n",
    "                \n",
    "                if negative_model=='bartlarge+bartneg':\n",
    "                    if not nb_to_train_on_feverous_all=='0':\n",
    "                        tmp+=[{'modeltouse':-2,'dsnamelocal':'raw_dataset_random_neg','pathtxt':'','maxnb':'','step':'test','localmodel':True},\n",
    "                            {'modeltouse':1 if fever_ft_all else negative_model_real,'pathtxt':'','dsnamelocal':'feverous_neg','maxnb':nb_to_train_on_feverous_all,'step':'train','localmodel':fever_ft_all},\n",
    "                            {'modeltouse':-1,'dsnamelocal':'dstopredictfromrandomnewneg','maxnb':'','pathtxt':-2,'step':'test','output':'neg','localmodel':True, 'change_subset':subsetname} ]\n",
    "                    else:\n",
    "                        tmp+=[{'modeltouse':0 if fever_ft_all else positive_model_real,'dsnamelocal':'raw_dataset_random_neg','pathtxt':'','maxnb':'','step':'test','localmodel':fever_ft_all},\n",
    "                            {'modeltouse':1 if fever_ft_all else negative_model_real,'dsnamelocal':'dstopredictfromrandomnewneg','pathtxt':'','maxnb':'','pathtxt':-1,'step':'test','output':'neg','localmodel':fever_ft_all, 'change_subset':subsetname} ]\n",
    "                    \n",
    "                else:\n",
    "                    if not nb_to_train_on_feverous_all=='0':\n",
    "                        tmp+=[{'modeltouse':1 if fever_ft_all else negative_model_real,'dsnamelocal':'feverous_neg','pathtxt':'','maxnb':nb_to_train_on_feverous_all,'step':'train','localmodel':fever_ft_all},\n",
    "                            {'modeltouse':-1,'dsnamelocal':'dstopredictfromrandomnewneg','pathtxt':'','maxnb':'','step':'test','output':'neg','localmodel':True, 'change_subset':subsetname}]\n",
    "                    else:\n",
    "                        tmp+=[{'modeltouse':1 if fever_ft_all else negative_model_real,'dsnamelocal':'dstopredictfromrandomnewneg','pathtxt':'','maxnb':'','step':'test','output':'neg','localmodel':fever_ft_all, 'change_subset':subsetname}]\n",
    "               \n",
    "                tmp2=copy.deepcopy(tmp)\n",
    "                dico_possibilities+=[{'feverftall':fever_ft_all,'nbtotrainonfeverousall':nb_to_train_on_feverous_all,'positivemodel':positive_model,'negativemodel':negative_model,'listexecs':tmp2}]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUUUUUUUUUUUUUUUUUUUUUUUUUUU0\n",
      "$$$$$$$$$$$$$$$$$$$\n",
      "{'feverftall': False, 'nbtotrainonfeverousall': '0', 'positivemodel': 'bartlarge', 'negativemodel': 'bartbase-WikiFactEnglish', 'listexecs': [{'modeltouse': 'facebook/bart-large', 'dsnamelocal': 'dstopredictfromrandomnewpos', 'pathtxt': '', 'maxnb': '', 'step': 'test', 'output': 'pos', 'localmodel': False, 'change_subset': 'sci_claims_randomevbartneg_cold'}, {'modeltouse': 'minwhoo/bart-base-negative-claim-generation', 'dsnamelocal': 'dstopredictfromrandomnewneg', 'pathtxt': '', 'maxnb': '', 'step': 'test', 'output': 'neg', 'localmodel': False, 'change_subset': 'sci_claims_randomevbartneg_cold'}]}\n",
      "$$$$$$$$$$$$$$$$$$$\n",
      "ds_modeltouse_facebookZbart-large_dsnamelocal_dstopredictfromrandomnewpos_step_test_change_subset_sciTclaimsTrandomevbartnegTcold hash : 0f06f86ef8b8bf257adae5a12d39d9d28ba4a2a2\n",
      "èèèèèè output type:pos\n",
      "##########\n",
      "['chmod a+wrx run_auto.sh', '/homes//feverous_work/feverousdata/']\n",
      "b''\n",
      "None\n",
      "##########\n",
      "['./run_auto.sh', '/homes//feverous_work/feverousdata/']\n",
      "b'\\rRunning tokenizer on prediction dataset:   0%|          | 0/237 [00:00<?, ? examples/s]\\r                                                                                       \\r/home//.conda/envs/feverous2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\\n  warnings.warn(\\nTEST: 237 samples\\n\\n\\r  0%|          | 0/30 [00:00<?, ?it/s]\\r  7%|\\xe2\\x96\\x8b         | 2/30 [00:02<00:29,  1.06s/it]\\r 10%|\\xe2\\x96\\x88         | 3/30 [00:04<00:40,  1.52s/it]\\r 13%|\\xe2\\x96\\x88\\xe2\\x96\\x8e        | 4/30 [00:06<00:47,  1.82s/it]\\r 17%|\\xe2\\x96\\x88\\xe2\\x96\\x8b        | 5/30 [00:08<00:48,  1.95s/it]\\r 20%|\\xe2\\x96\\x88\\xe2\\x96\\x88        | 6/30 [00:10<00:47,  2.00s/it]\\r 23%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e       | 7/30 [00:12<00:45,  2.00s/it]\\r 27%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b       | 8/30 [00:15<00:47,  2.14s/it]\\r 30%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88       | 9/30 [00:17<00:45,  2.15s/it]\\r 33%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e      | 10/30 [00:19<00:43,  2.15s/it]\\r 37%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b      | 11/30 [00:21<00:39,  2.10s/it]\\r 40%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88      | 12/30 [00:23<00:37,  2.10s/it]\\r 43%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e     | 13/30 [00:25<00:36,  2.13s/it]\\r 47%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b     | 14/30 [00:28<00:34,  2.13s/it]\\r 50%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88     | 15/30 [00:30<00:32,  2.16s/it]\\r 53%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e    | 16/30 [00:32<00:29,  2.11s/it]\\r 57%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b    | 17/30 [00:34<00:27,  2.08s/it]\\r 60%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88    | 18/30 [00:36<00:25,  2.13s/it]\\r 63%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e   | 19/30 [00:38<00:22,  2.07s/it]\\r 67%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b   | 20/30 [00:40<00:20,  2.04s/it]\\r 70%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88   | 21/30 [00:42<00:18,  2.06s/it]\\r 73%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e  | 22/30 [00:44<00:16,  2.08s/it]\\r 77%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b  | 23/30 [00:46<00:14,  2.07s/it]\\r 80%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88  | 24/30 [00:48<00:12,  2.08s/it]\\r 83%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e | 25/30 [00:50<00:10,  2.06s/it]\\r 87%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b | 26/30 [00:52<00:08,  2.05s/it]\\r 90%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88 | 27/30 [00:55<00:06,  2.07s/it]\\r 93%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e| 28/30 [00:57<00:04,  2.13s/it]\\r 97%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b| 29/30 [00:59<00:02,  2.09s/it]\\r100%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88| 30/30 [01:01<00:00,  1.99s/it]\\r100%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88| 30/30 [01:01<00:00,  2.03s/it]\\n***** predict metrics *****\\n  predict_runtime            = 0:01:04.19\\n  predict_samples            =        237\\n  predict_samples_per_second =      3.692\\n  predict_steps_per_second   =      0.467\\n'\n",
      "None\n",
      "ds_modeltouse_minwhooZbart-base-negative-claim-generation_dsnamelocal_dstopredictfromrandomnewneg_step_test_change_subset_sciTclaimsTrandomevbartnegTcold hash : 9ba19a3d6183a574433129cc4d48e6d787cffa95\n",
      "èèèèèè output type:neg\n",
      "##########\n",
      "['chmod a+wrx run_auto.sh', '/homes//feverous_work/feverousdata/']\n",
      "b''\n",
      "None\n",
      "##########\n",
      "['./run_auto.sh', '/homes//feverous_work/feverousdata/']\n",
      "b'\\rRunning tokenizer on prediction dataset:   0%|          | 0/237 [00:00<?, ? examples/s]\\r                                                                                       \\r/home//.conda/envs/feverous2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\\n  warnings.warn(\\nTEST: 237 samples\\n\\n\\r  0%|          | 0/30 [00:00<?, ?it/s]\\r  7%|\\xe2\\x96\\x8b         | 2/30 [00:01<00:18,  1.54it/s]\\r 10%|\\xe2\\x96\\x88         | 3/30 [00:02<00:27,  1.00s/it]\\r 13%|\\xe2\\x96\\x88\\xe2\\x96\\x8e        | 4/30 [00:04<00:28,  1.11s/it]\\r 17%|\\xe2\\x96\\x88\\xe2\\x96\\x8b        | 5/30 [00:05<00:29,  1.18s/it]\\r 20%|\\xe2\\x96\\x88\\xe2\\x96\\x88        | 6/30 [00:06<00:29,  1.21s/it]\\r 23%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e       | 7/30 [00:08<00:30,  1.32s/it]\\r 27%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b       | 8/30 [00:09<00:29,  1.34s/it]\\r 30%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88       | 9/30 [00:11<00:28,  1.37s/it]\\r 33%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e      | 10/30 [00:12<00:27,  1.35s/it]\\r 37%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b      | 11/30 [00:13<00:25,  1.34s/it]\\r 40%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88      | 12/30 [00:14<00:23,  1.32s/it]\\r 43%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e     | 13/30 [00:16<00:21,  1.28s/it]\\r 47%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b     | 14/30 [00:17<00:20,  1.28s/it]\\r 50%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88     | 15/30 [00:18<00:18,  1.26s/it]\\r 53%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e    | 16/30 [00:19<00:17,  1.28s/it]\\r 57%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b    | 17/30 [00:21<00:16,  1.29s/it]\\r 60%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88    | 18/30 [00:22<00:15,  1.29s/it]\\r 63%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e   | 19/30 [00:23<00:14,  1.31s/it]\\r 67%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b   | 20/30 [00:25<00:14,  1.41s/it]\\r 70%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88   | 21/30 [00:26<00:12,  1.35s/it]\\r 73%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e  | 22/30 [00:28<00:10,  1.34s/it]\\r 77%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b  | 23/30 [00:29<00:08,  1.27s/it]\\r 80%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88  | 24/30 [00:30<00:07,  1.22s/it]\\r 83%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e | 25/30 [00:31<00:06,  1.21s/it]\\r 87%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b | 26/30 [00:32<00:04,  1.21s/it]\\r 90%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88 | 27/30 [00:33<00:03,  1.21s/it]\\r 93%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e| 28/30 [00:35<00:02,  1.23s/it]\\r 97%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b| 29/30 [00:36<00:01,  1.21s/it]\\r100%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88| 30/30 [00:37<00:00,  1.15s/it]\\r100%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88| 30/30 [00:37<00:00,  1.24s/it]\\n***** predict metrics *****\\n  predict_runtime            = 0:00:39.85\\n  predict_samples            =        237\\n  predict_samples_per_second =      5.947\\n  predict_steps_per_second   =      0.753\\n'\n",
      "None\n",
      "list_execs\n",
      "[{'modeltouse': 'facebook/bart-large', 'dsnamelocal': 'dstopredictfromrandomnewpos', 'pathtxt': '', 'maxnb': '', 'step': 'test', 'output': 'pos', 'localmodel': False, 'change_subset': 'sci_claims_randomevbartneg_cold', 'outputdir': '0f06f86ef8b8bf257adae5a12d39d9d28ba4a2a2'}, {'modeltouse': 'minwhoo/bart-base-negative-claim-generation', 'dsnamelocal': 'dstopredictfromrandomnewneg', 'pathtxt': '', 'maxnb': '', 'step': 'test', 'output': 'neg', 'localmodel': False, 'change_subset': 'sci_claims_randomevbartneg_cold', 'outputdir': '9ba19a3d6183a574433129cc4d48e6d787cffa95'}]\n",
      "outputs\n",
      "[{'modeltouse': 'facebook/bart-large', 'dsnamelocal': 'dstopredictfromrandomnewpos', 'pathtxt': '', 'maxnb': '', 'step': 'test', 'output': 'pos', 'localmodel': False, 'change_subset': 'sci_claims_randomevbartneg_cold', 'outputdir': '0f06f86ef8b8bf257adae5a12d39d9d28ba4a2a2'}, [b'', b'\\rRunning tokenizer on prediction dataset:   0%|          | 0/237 [00:00<?, ? examples/s]\\r                                                                                       \\r/home//.conda/envs/feverous2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\\n  warnings.warn(\\nTEST: 237 samples\\n\\n\\r  0%|          | 0/30 [00:00<?, ?it/s]\\r  7%|\\xe2\\x96\\x8b         | 2/30 [00:02<00:29,  1.06s/it]\\r 10%|\\xe2\\x96\\x88         | 3/30 [00:04<00:40,  1.52s/it]\\r 13%|\\xe2\\x96\\x88\\xe2\\x96\\x8e        | 4/30 [00:06<00:47,  1.82s/it]\\r 17%|\\xe2\\x96\\x88\\xe2\\x96\\x8b        | 5/30 [00:08<00:48,  1.95s/it]\\r 20%|\\xe2\\x96\\x88\\xe2\\x96\\x88        | 6/30 [00:10<00:47,  2.00s/it]\\r 23%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e       | 7/30 [00:12<00:45,  2.00s/it]\\r 27%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b       | 8/30 [00:15<00:47,  2.14s/it]\\r 30%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88       | 9/30 [00:17<00:45,  2.15s/it]\\r 33%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e      | 10/30 [00:19<00:43,  2.15s/it]\\r 37%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b      | 11/30 [00:21<00:39,  2.10s/it]\\r 40%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88      | 12/30 [00:23<00:37,  2.10s/it]\\r 43%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e     | 13/30 [00:25<00:36,  2.13s/it]\\r 47%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b     | 14/30 [00:28<00:34,  2.13s/it]\\r 50%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88     | 15/30 [00:30<00:32,  2.16s/it]\\r 53%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e    | 16/30 [00:32<00:29,  2.11s/it]\\r 57%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b    | 17/30 [00:34<00:27,  2.08s/it]\\r 60%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88    | 18/30 [00:36<00:25,  2.13s/it]\\r 63%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e   | 19/30 [00:38<00:22,  2.07s/it]\\r 67%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b   | 20/30 [00:40<00:20,  2.04s/it]\\r 70%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88   | 21/30 [00:42<00:18,  2.06s/it]\\r 73%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e  | 22/30 [00:44<00:16,  2.08s/it]\\r 77%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b  | 23/30 [00:46<00:14,  2.07s/it]\\r 80%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88  | 24/30 [00:48<00:12,  2.08s/it]\\r 83%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e | 25/30 [00:50<00:10,  2.06s/it]\\r 87%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b | 26/30 [00:52<00:08,  2.05s/it]\\r 90%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88 | 27/30 [00:55<00:06,  2.07s/it]\\r 93%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e| 28/30 [00:57<00:04,  2.13s/it]\\r 97%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b| 29/30 [00:59<00:02,  2.09s/it]\\r100%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88| 30/30 [01:01<00:00,  1.99s/it]\\r100%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88| 30/30 [01:01<00:00,  2.03s/it]\\n***** predict metrics *****\\n  predict_runtime            = 0:01:04.19\\n  predict_samples            =        237\\n  predict_samples_per_second =      3.692\\n  predict_steps_per_second   =      0.467\\n'], {'modeltouse': 'minwhoo/bart-base-negative-claim-generation', 'dsnamelocal': 'dstopredictfromrandomnewneg', 'pathtxt': '', 'maxnb': '', 'step': 'test', 'output': 'neg', 'localmodel': False, 'change_subset': 'sci_claims_randomevbartneg_cold', 'outputdir': '9ba19a3d6183a574433129cc4d48e6d787cffa95'}, [b'', b'\\rRunning tokenizer on prediction dataset:   0%|          | 0/237 [00:00<?, ? examples/s]\\r                                                                                       \\r/home//.conda/envs/feverous2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\\n  warnings.warn(\\nTEST: 237 samples\\n\\n\\r  0%|          | 0/30 [00:00<?, ?it/s]\\r  7%|\\xe2\\x96\\x8b         | 2/30 [00:01<00:18,  1.54it/s]\\r 10%|\\xe2\\x96\\x88         | 3/30 [00:02<00:27,  1.00s/it]\\r 13%|\\xe2\\x96\\x88\\xe2\\x96\\x8e        | 4/30 [00:04<00:28,  1.11s/it]\\r 17%|\\xe2\\x96\\x88\\xe2\\x96\\x8b        | 5/30 [00:05<00:29,  1.18s/it]\\r 20%|\\xe2\\x96\\x88\\xe2\\x96\\x88        | 6/30 [00:06<00:29,  1.21s/it]\\r 23%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e       | 7/30 [00:08<00:30,  1.32s/it]\\r 27%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b       | 8/30 [00:09<00:29,  1.34s/it]\\r 30%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88       | 9/30 [00:11<00:28,  1.37s/it]\\r 33%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e      | 10/30 [00:12<00:27,  1.35s/it]\\r 37%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b      | 11/30 [00:13<00:25,  1.34s/it]\\r 40%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88      | 12/30 [00:14<00:23,  1.32s/it]\\r 43%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e     | 13/30 [00:16<00:21,  1.28s/it]\\r 47%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b     | 14/30 [00:17<00:20,  1.28s/it]\\r 50%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88     | 15/30 [00:18<00:18,  1.26s/it]\\r 53%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e    | 16/30 [00:19<00:17,  1.28s/it]\\r 57%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b    | 17/30 [00:21<00:16,  1.29s/it]\\r 60%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88    | 18/30 [00:22<00:15,  1.29s/it]\\r 63%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e   | 19/30 [00:23<00:14,  1.31s/it]\\r 67%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b   | 20/30 [00:25<00:14,  1.41s/it]\\r 70%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88   | 21/30 [00:26<00:12,  1.35s/it]\\r 73%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e  | 22/30 [00:28<00:10,  1.34s/it]\\r 77%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b  | 23/30 [00:29<00:08,  1.27s/it]\\r 80%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88  | 24/30 [00:30<00:07,  1.22s/it]\\r 83%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e | 25/30 [00:31<00:06,  1.21s/it]\\r 87%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b | 26/30 [00:32<00:04,  1.21s/it]\\r 90%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88 | 27/30 [00:33<00:03,  1.21s/it]\\r 93%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8e| 28/30 [00:35<00:02,  1.23s/it]\\r 97%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x8b| 29/30 [00:36<00:01,  1.21s/it]\\r100%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88| 30/30 [00:37<00:00,  1.15s/it]\\r100%|\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88\\xe2\\x96\\x88| 30/30 [00:37<00:00,  1.24s/it]\\n***** predict metrics *****\\n  predict_runtime            = 0:00:39.85\\n  predict_samples            =        237\\n  predict_samples_per_second =      5.947\\n  predict_steps_per_second   =      0.753\\n']]\n",
      "errors\n",
      "[{'modeltouse': 'facebook/bart-large', 'dsnamelocal': 'dstopredictfromrandomnewpos', 'pathtxt': '', 'maxnb': '', 'step': 'test', 'output': 'pos', 'localmodel': False, 'change_subset': 'sci_claims_randomevbartneg_cold', 'outputdir': '0f06f86ef8b8bf257adae5a12d39d9d28ba4a2a2'}, [...], {'modeltouse': 'minwhoo/bart-base-negative-claim-generation', 'dsnamelocal': 'dstopredictfromrandomnewneg', 'pathtxt': '', 'maxnb': '', 'step': 'test', 'output': 'neg', 'localmodel': False, 'change_subset': 'sci_claims_randomevbartneg_cold', 'outputdir': '9ba19a3d6183a574433129cc4d48e6d787cffa95'}, [...]]\n",
      "{'feverftall': False, 'nbtotrainonfeverousall': '0', 'positivemodel': 'bartlarge', 'negativemodel': 'bartbase-WikiFactEnglish', 'listexecs': [{'modeltouse': 'facebook/bart-large', 'dsnamelocal': 'dstopredictfromrandomnewpos', 'pathtxt': '', 'maxnb': '', 'step': 'test', 'output': 'pos', 'localmodel': False, 'change_subset': 'sci_claims_randomevbartneg_cold', 'outputdir': '0f06f86ef8b8bf257adae5a12d39d9d28ba4a2a2'}, {'modeltouse': 'minwhoo/bart-base-negative-claim-generation', 'dsnamelocal': 'dstopredictfromrandomnewneg', 'pathtxt': '', 'maxnb': '', 'step': 'test', 'output': 'neg', 'localmodel': False, 'change_subset': 'sci_claims_randomevbartneg_cold', 'outputdir': '9ba19a3d6183a574433129cc4d48e6d787cffa95'}]}\n",
      "file already done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Output folder name\n",
    "testablefolder='jf251023_txt_scifact_randomev_bartneg_cold'\n",
    "\n",
    "#For each experiment\n",
    "for nb_exec, elt in enumerate(dico_possibilities):\n",
    "    print(f'UUUUUUUUUUUUUUUUUUUUUUUUUUUU{nb_exec}')\n",
    "    print('$$$$$$$$$$$$$$$$$$$')\n",
    "    print(elt)\n",
    "    print('$$$$$$$$$$$$$$$$$$$')\n",
    "    #We run the whole pipeline of executions for this experiment\n",
    "    # We get the path to prediction and total_time\n",
    "    outposname,outnegname, total_time=pipeline_training_claimgenerator(elt['listexecs'])\n",
    "    # We convert the predictions to the runnable file\n",
    "    \n",
    "    ##############\n",
    "    \n",
    "         \n",
    "    print(elt)\n",
    "    \n",
    "    nc=outposname\n",
    "    ncn=outnegname\n",
    "    new_claims_pos=[]\n",
    "    new_claims_neg=[]\n",
    "    f_newclaims=open(nc)\n",
    "\n",
    "    f_newclaims_neg=open(ncn)\n",
    "\n",
    "    pos_same_seed_neg=False\n",
    "\n",
    "    #we create the input file in the scifact format\n",
    "    if not os.path.exists(base_path+'/'+testablefolder):\n",
    "        os.mkdir(base_path+'/'+testablefolder)\n",
    "        \n",
    "    for line in f_newclaims:\n",
    "        new_claims_pos+=[line.replace('\\n','')]\n",
    "\n",
    "    for line in f_newclaims_neg:\n",
    "        new_claims_neg+=[line.replace('\\n','')]\n",
    "    for dstom,claimstouse,label in [[respos,new_claims_pos,'SUPPORTS'],[resneg,new_claims_neg,'REFUTES']]:\n",
    "        for nb,elt_c in enumerate(claimstouse):\n",
    "            if 'claim' in claimstouse:\n",
    "                dstom[nb]['original_claim']=dstom[nb]['claim']\n",
    "            dstom[nb]['claim']=elt_c\n",
    "            dstom[nb]['label']=label\n",
    "    resf=respos+resneg\n",
    "\n",
    "    if elt['nbtotrainonfeverousall']=='':\n",
    "        elt['nbtotrainonfeverousall']='all'\n",
    "\n",
    "    #We create a parameters file containing information on the claims generated\n",
    "    params_to_write=copy.deepcopy(elt)\n",
    "    \n",
    "\n",
    "    params_to_write['pos_len']=len([x for x in resf if x['label']=='SUPPORTS'])\n",
    "    params_to_write['total_len']=params_to_write['pos_len'] if pos_same_seed_neg else len(resf)\n",
    "    params_to_write['neg_len']=len([x for x in resf if x['label']=='REFUTES'])\n",
    "    params_to_write['randomize_from_fileseed']=randomize_from_fileseed\n",
    "    params_to_write['randomize_strategy']=randomize_strategy\n",
    "    params_to_write['randomize_nbevidence']=randomize_nbevidence\n",
    "    params_to_write['outposname']=outposname\n",
    "    params_to_write['outnegname']=outnegname\n",
    "    \n",
    "    base_fn='ftfever_'+('1' if elt['feverftall'] else '0') + '_feveroustrainnb_'+str(elt['nbtotrainonfeverousall'])+'_'+elt['positivemodel']+'_'+str(params_to_write['pos_len'])+'_pos_'+elt['negativemodel']+'_'+str(params_to_write['neg_len'])\n",
    "    if os.path.exists(base_path+'/'+testablefolder+'/'+base_fn+'_params.json'):\n",
    "        print('file already done')\n",
    "    else:\n",
    "        params_to_write['total_time']=total_time\n",
    "        params_to_write['supportnli']='NA'\n",
    "        params_to_write['refutesnli']='NA'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        f4=open(base_path+'/'+testablefolder+'/'+base_fn+'_params.json','w')\n",
    "        json.dump(params_to_write,f4)\n",
    "\n",
    "        f4.close()\n",
    "\n",
    "\n",
    "        f4=open(base_path+'/'+testablefolder+'/'+base_fn+'.jsonl','w')\n",
    "        for eltb in resf:\n",
    "            f4.write(json.dumps(eltb)+'\\n')\n",
    "\n",
    "        f4.close()\n",
    "    \n",
    "   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
